{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(116.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18966, 4742)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.transforms import  ToTensor, Normalize, Compose, Resize, Grayscale\n",
    "\n",
    "custom_transform = Compose([\n",
    "    Resize((128, 128)),\n",
    "    Grayscale(),\n",
    "    ToTensor(),\n",
    "    # Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "age_list = [img.stem.split(\"_\")[0] for img in Path(\"data/train\").glob(\"*/*.jpg\")]\n",
    "age_min = 1 # torch.Tensor(list(map(int, age_list))).min()\n",
    "age_max = 117 # torch.Tensor(list(map(int, age_list))).max()\n",
    "print(age_min, age_max)\n",
    "target_transform = lambda x: (x - age_min)/(age_max - age_min)\n",
    "\n",
    "class create_dataset(Dataset):\n",
    "    def __init__(self, root: str, transform = custom_transform, target_transform = target_transform):\n",
    "        self.data = list(Path(root).glob(\"*/*.jpg\"))\n",
    "        \n",
    "        # normalizing the input data\n",
    "        self.age = [img.stem.split(\"_\")[0] for img in self.data]\n",
    "        self.age = torch.Tensor(list(map(int, self.age)))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.data[index]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.transform(image)\n",
    "        age = self.target_transform(self.age[index])\n",
    "        return image, age\n",
    "    \n",
    "train_dir = \"data/train\"\n",
    "test_dir = \"data/test\"\n",
    "\n",
    "train_data = create_dataset(train_dir, custom_transform, target_transform)\n",
    "test_data = create_dataset(test_dir, custom_transform, target_transform)\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8609)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 75)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class model_customV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.age_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128*16*16, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=256, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.conv_block_1(x)\n",
    "        # # print(x[:5])\n",
    "        # x = self.conv_block_2(x)\n",
    "        # # print(x[:5])\n",
    "        # x = self.conv_block_3(x)\n",
    "        # # print(x[:5])\n",
    "        # age = self.age_fc(x)\n",
    "        age = self.age_fc(self.conv_block_3(self.conv_block_2(self.conv_block_1(x))))\n",
    "        return age\n",
    "    \n",
    "\n",
    "model_age = model_customV0(input_shape=1, output_shape=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(params = model_age.parameters(),\n",
    "                           lr = 0.003,\n",
    "                        #    weight_decay=0.0001\n",
    "                           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0420, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_dataloader))\n",
    "\n",
    "y_hat = model_age(x.to(device))\n",
    "age_loss_fn(y_hat, y.type(torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device= device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.unsqueeze(1).to(device)\n",
    "        \n",
    "        y_preds = model(X)\n",
    "#         print(y.dtype, y_preds.dtype, y_pred_logits.dtype)\n",
    "#         print(y_pred_logits)\n",
    "#         print(y.shape, y_preds.shape, y_pred_logits.shape)\n",
    "\n",
    "        loss = loss_fn(y_preds, y.type(torch.float))\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    return train_loss.detach().cpu().numpy() # tensor.detach() to remove the grad associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device = device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.unsqueeze(1).to(device)\n",
    "\n",
    "            test_preds = model(X)\n",
    "            \n",
    "            loss = loss_fn(test_preds, y.type(torch.float))\n",
    "            test_loss += loss\n",
    "\n",
    "\n",
    "        test_loss /= len(dataloader)\n",
    "    return test_loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model:torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
    "          epochs = 5,\n",
    "          device = device):\n",
    "\n",
    "    results = {\"train loss\": [],\n",
    "               \"test loss\": []}\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model,\n",
    "                                        dataloader=train_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device)\n",
    "\n",
    "        test_loss = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        print(f\"train loss {train_loss:.4f} | test loss {test_loss:.4f} | \")\n",
    "        print(\"*\"*14)\n",
    "\n",
    "        # train_losses =\n",
    "\n",
    "        if results['test loss']:\n",
    "            if test_loss < min(results['test loss']):\n",
    "                torch.save(model.state_dict(), \"models/model_age.pt\")\n",
    "\n",
    "        results['train loss'].append(train_loss)\n",
    "        results['test loss'].append(test_loss)        \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3425b0473c344153940769ed53201cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.0111 | test loss 0.0116 | \n",
      "**************\n",
      "train loss 0.0111 | test loss 0.0116 | \n",
      "**************\n",
      "train loss 0.0110 | test loss 0.0116 | \n",
      "**************\n",
      "train loss 0.0110 | test loss 0.0116 | \n",
      "**************\n",
      "train loss 0.0110 | test loss 0.0115 | \n",
      "**************\n",
      "train loss 0.0109 | test loss 0.0118 | \n",
      "**************\n",
      "train loss 0.0108 | test loss 0.0118 | \n",
      "**************\n",
      "train loss 0.0107 | test loss 0.0114 | \n",
      "**************\n",
      "train loss 0.0108 | test loss 0.0113 | \n",
      "**************\n",
      "train loss 0.0107 | test loss 0.0112 | \n",
      "**************\n",
      "train loss 0.0106 | test loss 0.0112 | \n",
      "**************\n",
      "train loss 0.0107 | test loss 0.0112 | \n",
      "**************\n",
      "train loss 0.0106 | test loss 0.0113 | \n",
      "**************\n",
      "train loss 0.0106 | test loss 0.0112 | \n",
      "**************\n",
      "train loss 0.0105 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0105 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0104 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0104 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0103 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0103 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0102 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0102 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0102 | test loss 0.0109 | \n",
      "**************\n",
      "train loss 0.0101 | test loss 0.0114 | \n",
      "**************\n",
      "train loss 0.0101 | test loss 0.0109 | \n",
      "**************\n",
      "train loss 0.0101 | test loss 0.0109 | \n",
      "**************\n",
      "train loss 0.0101 | test loss 0.0107 | \n",
      "**************\n",
      "train loss 0.0100 | test loss 0.0107 | \n",
      "**************\n",
      "train loss 0.0099 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0100 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0099 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0099 | test loss 0.0107 | \n",
      "**************\n",
      "train loss 0.0100 | test loss 0.0105 | \n",
      "**************\n",
      "train loss 0.0098 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0098 | test loss 0.0127 | \n",
      "**************\n",
      "train loss 0.0098 | test loss 0.0105 | \n",
      "**************\n",
      "train loss 0.0097 | test loss 0.0106 | \n",
      "**************\n",
      "train loss 0.0097 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0096 | test loss 0.0105 | \n",
      "**************\n",
      "train loss 0.0096 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0096 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0096 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0096 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0095 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0095 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0095 | test loss 0.0113 | \n",
      "**************\n",
      "train loss 0.0094 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0094 | test loss 0.0102 | \n",
      "**************\n",
      "train loss 0.0094 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0094 | test loss 0.0101 | \n",
      "**************\n",
      "train loss 0.0094 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0092 | test loss 0.0101 | \n",
      "**************\n",
      "train loss 0.0093 | test loss 0.0103 | \n",
      "**************\n",
      "train loss 0.0093 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0093 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0092 | test loss 0.0114 | \n",
      "**************\n",
      "train loss 0.0091 | test loss 0.0100 | \n",
      "**************\n",
      "train loss 0.0092 | test loss 0.0109 | \n",
      "**************\n",
      "train loss 0.0091 | test loss 0.0100 | \n",
      "**************\n",
      "train loss 0.0091 | test loss 0.0099 | \n",
      "**************\n",
      "train loss 0.0091 | test loss 0.0099 | \n",
      "**************\n",
      "train loss 0.0091 | test loss 0.0099 | \n",
      "**************\n",
      "train loss 0.0090 | test loss 0.0099 | \n",
      "**************\n",
      "train loss 0.0090 | test loss 0.0098 | \n",
      "**************\n",
      "train loss 0.0090 | test loss 0.0100 | \n",
      "**************\n",
      "train loss 0.0090 | test loss 0.0108 | \n",
      "**************\n",
      "train loss 0.0089 | test loss 0.0098 | \n",
      "**************\n",
      "train loss 0.0089 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0089 | test loss 0.0098 | \n",
      "**************\n",
      "train loss 0.0089 | test loss 0.0100 | \n",
      "**************\n",
      "train loss 0.0088 | test loss 0.0115 | \n",
      "**************\n",
      "train loss 0.0089 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0088 | test loss 0.0110 | \n",
      "**************\n",
      "train loss 0.0088 | test loss 0.0109 | \n",
      "**************\n",
      "train loss 0.0088 | test loss 0.0111 | \n",
      "**************\n",
      "train loss 0.0087 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0087 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0086 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0087 | test loss 0.0106 | \n",
      "**************\n",
      "train loss 0.0086 | test loss 0.0095 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0086 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0107 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0129 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0095 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0094 | \n",
      "**************\n",
      "train loss 0.0085 | test loss 0.0094 | \n",
      "**************\n",
      "train loss 0.0084 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0084 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0084 | test loss 0.0096 | \n",
      "**************\n",
      "train loss 0.0084 | test loss 0.0100 | \n",
      "**************\n",
      "train loss 0.0083 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0084 | test loss 0.0094 | \n",
      "**************\n",
      "train loss 0.0083 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0083 | test loss 0.0093 | \n",
      "**************\n",
      "train loss 0.0082 | test loss 0.0099 | \n",
      "**************\n",
      "train loss 0.0082 | test loss 0.0097 | \n",
      "**************\n",
      "train loss 0.0082 | test loss 0.0104 | \n",
      "**************\n",
      "train loss 0.0082 | test loss 0.0095 | \n",
      "**************\n",
      "Total training time: 2340.6581209000014\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "model_results = train(model=model_age.to(device),\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=age_loss_fn,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"models\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_save_path = path / \"model_age.pt\"\n",
    "torch.save(model_age.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models\\\\AGEV0.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      4\u001b[0m model_age \u001b[38;5;241m=\u001b[39m model_customV0(input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 5\u001b[0m model_age\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAGEV0.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m rand_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_data), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      9\u001b[0m X, y \u001b[38;5;241m=\u001b[39m test_data[rand_idx]\n",
      "File \u001b[1;32mc:\\Users\\SAYAN\\OneDrive\\Desktop\\temp\\venv\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\SAYAN\\OneDrive\\Desktop\\temp\\venv\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\SAYAN\\OneDrive\\Desktop\\temp\\venv\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models\\\\AGEV0.pt'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "model_age = model_customV0(input_shape=1, output_shape=1).to(device=device)\n",
    "model_age.load_state_dict(torch.load(Path(\"models\") / \"AGEV0.pth\"))\n",
    "\n",
    "rand_idx = torch.randint(0, len(test_data), size=(1,)).item()\n",
    "\n",
    "X, y = test_data[rand_idx]\n",
    "X, y = X.unsqueeze(0).to(device), y.unsqueeze(0).to(device)\n",
    "\n",
    "model_age.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_age(X)\n",
    "\n",
    "plt.imshow(X.squeeze(0).permute(1, 2, 0).cpu())\n",
    "plt.title(f\"predicted: {int((y_preds.item() * (age_max - age_min)) + age_min)} | actual: {int((y * (age_max - age_min)) + age_min)}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train loss': [array(0.01112642, dtype=float32)],\n",
       " 'test loss': [array(0.01166613, dtype=float32)]}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
